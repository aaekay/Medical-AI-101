{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Module 4: Evaluating ML Models in Medicine\n## Beyond Accuracy: Thresholds, Capacity, and Clinical Trade-offs\n\n**Goal:** Learn how evaluation changes when model outputs are used for real decisions in a clinical workflow."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### How to use this notebook\n- Run cells from top to bottom.\n- Change thresholds and policy settings with widgets.\n- Focus on how decisions shift when the clinical context changes."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Learning objectives\n1. Compare model performance using sensitivity, specificity, precision, F1, ROC-AUC, and PR-AUC.\n2. Understand why threshold choice is a policy decision, not just a technical default.\n3. Simulate resource-limited triage where only top-risk patients can be flagged.\n4. Use simple cost assumptions to pick an operating threshold.\n5. Produce a transparent policy summary for team discussion."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 0: Clinical Problem\nA readmission model gives each patient a risk probability.\nThe clinical team must decide **where to set the action threshold** and **how many patients can receive interventions**.\nThis module focuses on the decision layer after modeling."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "Run this cell once at the start. It auto-configures paths in Google Colab and does nothing harmful on local Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def setup_repo_for_colab(\n",
    "    repo_url='https://github.com/aaekay/Medical-AI-101.git',\n",
    "    repo_dir='/content/Medical-AI-101',\n",
    "    notebook_dir='chapters',\n",
    "):\n",
    "    if 'google.colab' not in sys.modules:\n",
    "        print(f'Local runtime detected. Working directory: {Path.cwd()}')\n",
    "        return\n",
    "\n",
    "    repo_path = Path(repo_dir)\n",
    "    if not repo_path.exists():\n",
    "        print('Cloning Medical-AI-101 into /content ...')\n",
    "        subprocess.check_call(['git', 'clone', repo_url, str(repo_path)])\n",
    "\n",
    "    target = repo_path / notebook_dir\n",
    "    os.chdir(target)\n",
    "    print(f'Colab ready. Working directory: {Path.cwd()}')\n",
    "\n",
    "setup_repo_for_colab()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom IPython.display import display\nfrom sklearn.metrics import (\n    average_precision_score,\n    confusion_matrix,\n    precision_recall_curve,\n    roc_auc_score,\n    roc_curve,\n)\n\ntry:\n    import ipywidgets as widgets\nexcept ImportError as exc:\n    raise ImportError('ipywidgets is required for interactive demos in this notebook.') from exc\n\n\ndef resolve_data_path(filename):\n    candidates = [Path('../data') / filename, Path('data') / filename]\n    for cand in candidates:\n        if cand.exists():\n            return cand\n    raise FileNotFoundError(f'Could not locate {filename} in ../data or data/. Run Module 3 first.')\n\n\nPRED_PATH = resolve_data_path('module_03_test_predictions.csv')\ndf_pred = pd.read_csv(PRED_PATH)\nprint(f'Loaded {len(df_pred)} prediction rows from {PRED_PATH}.')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "required_cols = [\n    'encounter_id',\n    'actual_readmit_30d',\n    'lr_probability',\n    'tree_probability',\n]\nmissing = [c for c in required_cols if c not in df_pred.columns]\nif missing:\n    raise ValueError(f'Missing required columns: {missing}')\n\nfor col in ['lr_probability', 'tree_probability']:\n    if ((df_pred[col] < 0) | (df_pred[col] > 1)).any():\n        raise ValueError(f'{col} has values outside [0, 1].')\n\nprevalence = df_pred['actual_readmit_30d'].mean()\nprint(f\"Outcome prevalence in this test set: {prevalence:.3f}\")\ndisplay(df_pred.head(8))\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 1: Metrics at a Default Threshold (0.50)\nThis mirrors common quick evaluations, but we will soon show why fixed 0.50 is rarely enough in clinical operations."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "y_true = df_pred['actual_readmit_30d'].astype(int).values\nmodel_probs = {\n    'Logistic Regression': df_pred['lr_probability'].values,\n    'Decision Tree': df_pred['tree_probability'].values,\n}\n\n\ndef metric_bundle(y, proba, threshold):\n    pred = (proba >= threshold).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y, pred, labels=[0, 1]).ravel()\n\n    total = tp + tn + fp + fn\n    accuracy = (tp + tn) / total if total else np.nan\n    precision = tp / (tp + fp) if (tp + fp) else 0.0\n    sensitivity = tp / (tp + fn) if (tp + fn) else 0.0\n    specificity = tn / (tn + fp) if (tn + fp) else 0.0\n    f1 = 2 * precision * sensitivity / (precision + sensitivity) if (precision + sensitivity) else np.nan\n    roc_auc = roc_auc_score(y, proba)\n    pr_auc = average_precision_score(y, proba)\n\n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'sensitivity_recall': sensitivity,\n        'specificity': specificity,\n        'f1': f1,\n        'roc_auc': roc_auc,\n        'pr_auc': pr_auc,\n        'tp': tp,\n        'tn': tn,\n        'fp': fp,\n        'fn': fn,\n        'flag_rate': pred.mean(),\n    }\n\n\ndefault_rows = []\nfor model_name, proba in model_probs.items():\n    row = metric_bundle(y_true, proba, threshold=0.50)\n    row['model'] = model_name\n    default_rows.append(row)\n\ndefault_metrics = pd.DataFrame(default_rows).set_index('model')\ndisplay(default_metrics[['accuracy', 'precision', 'sensitivity_recall', 'specificity', 'f1', 'roc_auc', 'pr_auc', 'flag_rate']].round(3))\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 2: ROC and Precision-Recall Curves\nROC shows ranking performance; PR is often more informative when positives are less frequent."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(1, 2, figsize=(12, 4.2))\n\n# ROC\nfor model_name, proba in model_probs.items():\n    fpr, tpr, _ = roc_curve(y_true, proba)\n    axes[0].plot(fpr, tpr, linewidth=2, label=f\"{model_name} (AUC={roc_auc_score(y_true, proba):.2f})\")\naxes[0].plot([0, 1], [0, 1], '--', color='gray', linewidth=1)\naxes[0].set_title('ROC Curve')\naxes[0].set_xlabel('False Positive Rate')\naxes[0].set_ylabel('True Positive Rate (Sensitivity)')\naxes[0].legend()\n\n# PR\nbaseline = y_true.mean()\nfor model_name, proba in model_probs.items():\n    precision, recall, _ = precision_recall_curve(y_true, proba)\n    ap = average_precision_score(y_true, proba)\n    axes[1].plot(recall, precision, linewidth=2, label=f\"{model_name} (AP={ap:.2f})\")\naxes[1].hlines(baseline, 0, 1, linestyles='--', color='gray', linewidth=1, label=f'Prevalence={baseline:.2f}')\naxes[1].set_title('Precision-Recall Curve')\naxes[1].set_xlabel('Recall')\naxes[1].set_ylabel('Precision')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 3: Interactive Threshold Explorer\nMove the threshold and observe how clinical workload and miss rate change."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def threshold_explorer(model='Logistic Regression', threshold=0.50):\n    proba = model_probs[model]\n    m = metric_bundle(y_true, proba, threshold)\n\n    flagged_per_100 = m['flag_rate'] * 100\n    missed_per_100 = (m['fn'] / len(y_true)) * 100\n\n    print(f'Model: {model}')\n    print(f'Threshold: {threshold:.2f}')\n    print(\n        f\"Sensitivity: {m['sensitivity_recall']:.3f} | Specificity: {m['specificity']:.3f} | \"\n        f\"Precision: {m['precision']:.3f}\"\n    )\n    print(f'Flagged for intervention: {flagged_per_100:.1f} per 100 discharges')\n    print(f'Missed true readmissions (false negatives): {missed_per_100:.1f} per 100 discharges')\n\n    counts = pd.Series({'TP': m['tp'], 'TN': m['tn'], 'FP': m['fp'], 'FN': m['fn']})\n\n    fig, ax = plt.subplots(figsize=(6.2, 3.2))\n    ax.bar(counts.index, counts.values, color=['#54a24b', '#4c78a8', '#f58518', '#e45756'])\n    ax.set_title('Confusion Counts at Selected Threshold')\n    ax.set_ylabel('Patients')\n    plt.tight_layout()\n    plt.show()\n\n\nwidgets.interact(\n    threshold_explorer,\n    model=widgets.Dropdown(options=list(model_probs.keys()), value='Logistic Regression', description='Model'),\n    threshold=widgets.FloatSlider(value=0.50, min=0.05, max=0.95, step=0.05, description='Threshold', continuous_update=False),\n)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 4: Capacity-Constrained Triage\nSome teams can only review a fixed percentage of patients.\nHere we simulate a top-K strategy using highest predicted risk."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def topk_policy_metrics(y, proba, capacity_pct):\n    n = len(y)\n    k = max(1, int(np.ceil((capacity_pct / 100.0) * n)))\n\n    order = np.argsort(-proba)\n    flagged = np.zeros(n, dtype=int)\n    flagged[order[:k]] = 1\n\n    tn, fp, fn, tp = confusion_matrix(y, flagged, labels=[0, 1]).ravel()\n    recall = tp / (tp + fn) if (tp + fn) else 0.0\n    precision = tp / (tp + fp) if (tp + fp) else 0.0\n\n    return {\n        'capacity_pct': capacity_pct,\n        'k_flagged': k,\n        'tp': tp,\n        'fp': fp,\n        'fn': fn,\n        'tn': tn,\n        'recall': recall,\n        'precision': precision,\n        'flag_rate': flagged.mean(),\n    }\n\n\ndef capacity_explorer(model='Logistic Regression', capacity_pct=20):\n    proba = model_probs[model]\n    m = topk_policy_metrics(y_true, proba, capacity_pct)\n\n    print(f'Model: {model}')\n    print(f'Capacity: {capacity_pct:.0f}% of patients ({m[\"k_flagged\"]} out of {len(y_true)})')\n    print(f\"Recall captured by top-K: {m['recall']:.3f}\")\n    print(f\"Precision among flagged: {m['precision']:.3f}\")\n\n    bars = pd.Series({'TP captured': m['tp'], 'FP flagged': m['fp'], 'FN missed': m['fn']})\n    fig, ax = plt.subplots(figsize=(6.2, 3.2))\n    ax.bar(bars.index, bars.values, color=['#54a24b', '#f58518', '#e45756'])\n    ax.set_title('Top-K Capacity Policy Outcome Counts')\n    ax.set_ylabel('Patients')\n    ax.tick_params(axis='x', rotation=10)\n    plt.tight_layout()\n    plt.show()\n\n\nwidgets.interact(\n    capacity_explorer,\n    model=widgets.Dropdown(options=list(model_probs.keys()), value='Logistic Regression', description='Model'),\n    capacity_pct=widgets.IntSlider(value=20, min=5, max=80, step=5, description='Capacity %', continuous_update=False),\n)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 5: Utility-Based Threshold Selection\nChoose threshold by minimizing expected policy cost, where false negatives and false positives have different penalties."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def expected_cost(y, proba, threshold, fn_cost=10.0, fp_cost=1.0):\n    pred = (proba >= threshold).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y, pred, labels=[0, 1]).ravel()\n    total_cost = (fn * fn_cost) + (fp * fp_cost)\n    cost_per_patient = total_cost / len(y)\n    return cost_per_patient, {'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp}\n\n\ndef find_best_threshold(y, proba, fn_cost=10.0, fp_cost=1.0):\n    grid = np.round(np.arange(0.05, 0.96, 0.01), 2)\n    rows = []\n    for t in grid:\n        c, cm = expected_cost(y, proba, t, fn_cost=fn_cost, fp_cost=fp_cost)\n        m = metric_bundle(y, proba, t)\n        rows.append({\n            'threshold': t,\n            'cost_per_patient': c,\n            'sensitivity_recall': m['sensitivity_recall'],\n            'specificity': m['specificity'],\n            'precision': m['precision'],\n            'flag_rate': m['flag_rate'],\n            'tp': cm['tp'],\n            'fp': cm['fp'],\n            'fn': cm['fn'],\n            'tn': cm['tn'],\n        })\n    df_grid = pd.DataFrame(rows)\n    best = df_grid.loc[df_grid['cost_per_patient'].idxmin()]\n    return df_grid, best\n\n\ndef utility_explorer(model='Logistic Regression', fn_cost=10.0, fp_cost=1.0):\n    proba = model_probs[model]\n    grid, best = find_best_threshold(y_true, proba, fn_cost=fn_cost, fp_cost=fp_cost)\n\n    print(f'Model: {model}')\n    print(f'False negative cost: {fn_cost:.1f} | False positive cost: {fp_cost:.1f}')\n    print(f\"Best threshold by expected cost: {best['threshold']:.2f}\")\n    print(f\"Expected cost per patient: {best['cost_per_patient']:.3f}\")\n    print(\n        f\"Sensitivity: {best['sensitivity_recall']:.3f} | Specificity: {best['specificity']:.3f} | \"\n        f\"Precision: {best['precision']:.3f}\"\n    )\n\n    fig, ax = plt.subplots(figsize=(7, 3.5))\n    ax.plot(grid['threshold'], grid['cost_per_patient'], color='#4c78a8', linewidth=2)\n    ax.axvline(best['threshold'], color='#e45756', linestyle='--', label=f\"Best={best['threshold']:.2f}\")\n    ax.set_title('Expected Cost per Patient Across Thresholds')\n    ax.set_xlabel('Threshold')\n    ax.set_ylabel('Cost per patient')\n    ax.legend()\n    plt.tight_layout()\n    plt.show()\n\n\nwidgets.interact(\n    utility_explorer,\n    model=widgets.Dropdown(options=list(model_probs.keys()), value='Logistic Regression', description='Model'),\n    fn_cost=widgets.FloatSlider(value=10.0, min=1.0, max=20.0, step=1.0, description='FN cost', continuous_update=False),\n    fp_cost=widgets.FloatSlider(value=1.0, min=0.5, max=10.0, step=0.5, description='FP cost', continuous_update=False),\n)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 6: Scenario Table for Team Discussion\nThis creates a simple policy summary under three clinical scenarios."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "scenarios = [\n    {'scenario': 'Safety-first clinic', 'fn_cost': 12.0, 'fp_cost': 1.0},\n    {'scenario': 'Balanced operations', 'fn_cost': 6.0, 'fp_cost': 2.0},\n    {'scenario': 'Resource-limited service', 'fn_cost': 3.0, 'fp_cost': 3.0},\n]\n\nsummary_rows = []\nfor s in scenarios:\n    for model_name, proba in model_probs.items():\n        grid, best = find_best_threshold(y_true, proba, fn_cost=s['fn_cost'], fp_cost=s['fp_cost'])\n        summary_rows.append({\n            'scenario': s['scenario'],\n            'model': model_name,\n            'fn_cost': s['fn_cost'],\n            'fp_cost': s['fp_cost'],\n            'recommended_threshold': float(best['threshold']),\n            'expected_cost_per_patient': float(best['cost_per_patient']),\n            'sensitivity_recall': float(best['sensitivity_recall']),\n            'specificity': float(best['specificity']),\n            'precision': float(best['precision']),\n            'flag_rate': float(best['flag_rate']),\n        })\n\npolicy_summary = pd.DataFrame(summary_rows)\npolicy_summary = policy_summary.sort_values(['scenario', 'expected_cost_per_patient']).reset_index(drop=True)\ndisplay(policy_summary.round(3))\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 7: Save Policy Summary for Module 5\nThis file can be reused in later modules on workflow design, governance, and deployment decisions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "out_path = PRED_PATH.parent / 'module_04_threshold_policy_summary.csv'\npolicy_summary.to_csv(out_path, index=False)\nprint(f'Saved policy summary to {out_path}')\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Wrap-up: Key Takeaways\n- Good ranking performance does not automatically define a good clinical policy.\n- Threshold and capacity settings directly change workload and missed cases.\n- Costs and risk tolerance should be explicit and discussable with stakeholders.\n- A transparent policy table is often more useful than one headline metric."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
