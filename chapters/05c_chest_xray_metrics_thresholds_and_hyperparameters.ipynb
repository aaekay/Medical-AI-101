{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Module 5C: Metrics, Thresholds, and Hyperparameter Decisions\n## Turning Model Scores Into Clinical Action\n\n**Goal:** Use predictions from Module 5B to choose thresholds and hyperparameters based on clinical trade-offs, not just one accuracy number."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Learning objectives\n1. Compare ROC and PR curves for chest X-ray classification.\n2. Understand when PR-AUC is more informative than ROC-AUC.\n3. Select thresholds using sensitivity/specificity/workload constraints.\n4. Compare hyperparameter runs by validation outcomes.\n5. Export a simple model policy card for team discussion."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 0: Load Outputs From Module 5A and 5B\nRequired for full analysis:\n- `data/module_05b_test_predictions.csv`\n- `data/module_05b_training_history.csv`\n\nOptional baseline comparison:\n- `data/module_05a_baseline_test_predictions.csv`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom IPython.display import display\nfrom sklearn.metrics import (\n    average_precision_score,\n    confusion_matrix,\n    precision_recall_curve,\n    roc_auc_score,\n    roc_curve,\n)\n\ntry:\n    import ipywidgets as widgets\nexcept ImportError as exc:\n    raise ImportError('ipywidgets is required for Module 5C interactive controls.') from exc\n\n\ndef resolve_data_file(name):\n    candidates = [Path('../data') / name, Path('data') / name]\n    for cand in candidates:\n        if cand.exists():\n            return cand\n    return None\n\n\ncnn_test_path = resolve_data_file('module_05b_test_predictions.csv')\ncnn_hist_path = resolve_data_file('module_05b_training_history.csv')\nbaseline_path = resolve_data_file('module_05a_baseline_test_predictions.csv')\n\nif cnn_test_path is None:\n    print('Missing module_05b_test_predictions.csv. Run Module 5B first.')\n    cnn_test = pd.DataFrame(columns=['label', 'probability'])\nelse:\n    cnn_test = pd.read_csv(cnn_test_path)\n    print(f'Loaded CNN test predictions: {len(cnn_test)} rows')\n\nif cnn_hist_path is None:\n    cnn_history = pd.DataFrame()\n    print('No training history found yet. Run at least one Module 5B experiment.')\nelse:\n    cnn_history = pd.read_csv(cnn_hist_path)\n    print(f'Loaded training history rows: {len(cnn_history)}')\n\nif baseline_path is None:\n    baseline_test = pd.DataFrame()\n    print('Baseline file not found (optional).')\nelse:\n    baseline_test = pd.read_csv(baseline_path)\n    print(f'Loaded baseline predictions: {len(baseline_test)} rows')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not cnn_test.empty:\n    display(cnn_test.head())\n    prevalence = cnn_test['label'].mean()\n    print(f'Positive prevalence in test set: {prevalence:.3f}')\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 1: Metric Functions\nWe explicitly compute threshold-dependent and threshold-independent metrics."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def metric_bundle(y_true, y_prob, threshold=0.5):\n    y_pred = (y_prob >= threshold).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n\n    accuracy = (tp + tn) / (tp + tn + fp + fn)\n    precision = tp / (tp + fp) if (tp + fp) else 0.0\n    recall = tp / (tp + fn) if (tp + fn) else 0.0\n    specificity = tn / (tn + fp) if (tn + fp) else 0.0\n    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n\n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall_sensitivity': recall,\n        'specificity': specificity,\n        'f1': f1,\n        'tp': tp,\n        'tn': tn,\n        'fp': fp,\n        'fn': fn,\n        'flag_rate': y_pred.mean(),\n        'roc_auc': roc_auc_score(y_true, y_prob),\n        'pr_auc': average_precision_score(y_true, y_prob),\n    }\n\n\ndef threshold_grid(y_true, y_prob):\n    rows = []\n    for t in np.round(np.arange(0.05, 0.96, 0.01), 2):\n        m = metric_bundle(y_true, y_prob, threshold=t)\n        rows.append({'threshold': t, **m})\n    return pd.DataFrame(rows)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 2: PR vs ROC Comparison\nUse ROC-AUC for global ranking, and PR-AUC for positive-class retrieval focus.\nIn screening settings, PR often better reflects operational reality."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if cnn_test.empty:\n    print('No CNN predictions available for curve plotting.')\nelse:\n    y = cnn_test['label'].astype(int).values\n    p = cnn_test['probability'].values\n\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4.2))\n\n    fpr, tpr, _ = roc_curve(y, p)\n    axes[0].plot(fpr, tpr, color='#4c78a8', linewidth=2, label=f'CNN AUC={roc_auc_score(y, p):.2f}')\n    axes[0].plot([0, 1], [0, 1], '--', color='gray', linewidth=1)\n\n    if not baseline_test.empty and len(baseline_test) == len(cnn_test):\n        pb = baseline_test['probability'].values\n        fpr_b, tpr_b, _ = roc_curve(y, pb)\n        axes[0].plot(fpr_b, tpr_b, color='#e45756', linewidth=2, label=f'Baseline AUC={roc_auc_score(y, pb):.2f}')\n\n    axes[0].set_title('ROC Curve')\n    axes[0].set_xlabel('False Positive Rate')\n    axes[0].set_ylabel('True Positive Rate')\n    axes[0].legend()\n\n    precision, recall, _ = precision_recall_curve(y, p)\n    axes[1].plot(recall, precision, color='#4c78a8', linewidth=2, label=f'CNN AP={average_precision_score(y, p):.2f}')\n\n    if not baseline_test.empty and len(baseline_test) == len(cnn_test):\n        precision_b, recall_b, _ = precision_recall_curve(y, pb)\n        axes[1].plot(recall_b, precision_b, color='#e45756', linewidth=2, label=f'Baseline AP={average_precision_score(y, pb):.2f}')\n\n    axes[1].hlines(y.mean(), 0, 1, color='gray', linestyle='--', linewidth=1, label=f'Prevalence={y.mean():.2f}')\n    axes[1].set_title('Precision-Recall Curve')\n    axes[1].set_xlabel('Recall')\n    axes[1].set_ylabel('Precision')\n    axes[1].legend()\n\n    plt.tight_layout()\n    plt.show()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 3: Interactive Threshold Selection\nPick thresholds by clinical objective: sensitivity-first, precision-first, or balanced operations."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def threshold_explorer(threshold=0.50):\n    if cnn_test.empty:\n        print('No CNN predictions available.')\n        return\n\n    y = cnn_test['label'].astype(int).values\n    p = cnn_test['probability'].values\n    m = metric_bundle(y, p, threshold=threshold)\n\n    print(f'Threshold: {threshold:.2f}')\n    print(f\"Sensitivity: {m['recall_sensitivity']:.3f} | Specificity: {m['specificity']:.3f} | Precision: {m['precision']:.3f}\")\n    print(f\"PR-AUC: {m['pr_auc']:.3f} | ROC-AUC: {m['roc_auc']:.3f}\")\n    print(f\"Flagged workload: {m['flag_rate'] * 100:.1f} per 100 images\")\n\n    bars = pd.Series({'TP': m['tp'], 'TN': m['tn'], 'FP': m['fp'], 'FN': m['fn']})\n    fig, ax = plt.subplots(figsize=(6.2, 3.2))\n    ax.bar(bars.index, bars.values, color=['#54a24b', '#4c78a8', '#f58518', '#e45756'])\n    ax.set_title('CNN Confusion Counts')\n    ax.set_ylabel('Images')\n    plt.tight_layout()\n    plt.show()\n\n\nwidgets.interact(\n    threshold_explorer,\n    threshold=widgets.FloatSlider(value=0.50, min=0.05, max=0.95, step=0.05, description='Threshold', continuous_update=False),\n)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 4: Cost-Aware Threshold Recommendation\nIf missing pneumonia is more costly than false alarms, threshold should move lower."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def choose_threshold_by_cost(y_true, y_prob, fn_cost=10.0, fp_cost=1.0):\n    grid = threshold_grid(y_true, y_prob)\n    grid['expected_cost_per_image'] = (grid['fn'] * fn_cost + grid['fp'] * fp_cost) / len(y_true)\n    best = grid.loc[grid['expected_cost_per_image'].idxmin()]\n    return grid, best\n\n\ndef cost_explorer(fn_cost=10.0, fp_cost=1.0):\n    if cnn_test.empty:\n        print('No CNN predictions available.')\n        return\n\n    y = cnn_test['label'].astype(int).values\n    p = cnn_test['probability'].values\n    grid, best = choose_threshold_by_cost(y, p, fn_cost=fn_cost, fp_cost=fp_cost)\n\n    print(f'FN cost={fn_cost:.1f}, FP cost={fp_cost:.1f}')\n    print(f\"Recommended threshold: {best['threshold']:.2f}\")\n    print(f\"Sensitivity={best['recall_sensitivity']:.3f} | Specificity={best['specificity']:.3f} | Precision={best['precision']:.3f}\")\n    print(f\"Expected cost per image: {best['expected_cost_per_image']:.3f}\")\n\n    fig, ax = plt.subplots(figsize=(7, 3.5))\n    ax.plot(grid['threshold'], grid['expected_cost_per_image'], color='#4c78a8', linewidth=2)\n    ax.axvline(best['threshold'], color='#e45756', linestyle='--', label=f\"Best={best['threshold']:.2f}\")\n    ax.set_title('Expected Cost vs Threshold')\n    ax.set_xlabel('Threshold')\n    ax.set_ylabel('Cost per image')\n    ax.legend()\n    plt.tight_layout()\n    plt.show()\n\n\nwidgets.interact(\n    cost_explorer,\n    fn_cost=widgets.FloatSlider(value=10.0, min=1.0, max=20.0, step=1.0, description='FN cost', continuous_update=False),\n    fp_cost=widgets.FloatSlider(value=1.0, min=0.5, max=10.0, step=0.5, description='FP cost', continuous_update=False),\n)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 5: Hyperparameter Run Comparison\nCompare experiments from Module 5B to choose stable settings before final training."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if cnn_history.empty:\n    print('No training history to compare yet.')\nelse:\n    run_summary = (\n        cnn_history.sort_values('epoch')\n        .groupby('run_id', as_index=False)\n        .agg(\n            batch_size=('batch_size', 'last'),\n            img_size=('img_size', 'last'),\n            learning_rate=('learning_rate', 'last'),\n            weight_decay=('weight_decay', 'last'),\n            dropout=('dropout', 'last'),\n            epochs=('epochs', 'last'),\n            elapsed_sec=('elapsed_sec', 'last'),\n            best_val_roc_auc=('val_roc_auc', 'max'),\n            best_val_pr_auc=('val_pr_auc', 'max'),\n            final_val_loss=('val_loss', 'last'),\n        )\n        .sort_values('best_val_pr_auc', ascending=False)\n        .reset_index(drop=True)\n    )\n\n    display(run_summary.round(4))\n\n    fig, ax = plt.subplots(figsize=(7, 4))\n    sc = ax.scatter(\n        run_summary['best_val_roc_auc'],\n        run_summary['best_val_pr_auc'],\n        s=np.clip(run_summary['elapsed_sec'], 20, 400),\n        c=run_summary['dropout'],\n        cmap='viridis',\n        alpha=0.8,\n    )\n    ax.set_xlabel('Best Validation ROC-AUC')\n    ax.set_ylabel('Best Validation PR-AUC')\n    ax.set_title('Hyperparameter Run Comparison')\n    cbar = plt.colorbar(sc, ax=ax)\n    cbar.set_label('Dropout')\n    plt.tight_layout()\n    plt.show()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 6: Export Model Policy Card\nThis creates a compact table for team review and deployment planning."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if cnn_test.empty:\n    print('Cannot export policy card without CNN predictions.')\nelse:\n    y = cnn_test['label'].astype(int).values\n    p = cnn_test['probability'].values\n\n    scenarios = [\n        {'scenario': 'Safety-first screening', 'fn_cost': 12.0, 'fp_cost': 1.0},\n        {'scenario': 'Balanced workflow', 'fn_cost': 6.0, 'fp_cost': 2.0},\n        {'scenario': 'Resource-limited follow-up', 'fn_cost': 3.0, 'fp_cost': 3.0},\n    ]\n\n    rows = []\n    for s in scenarios:\n        grid, best = choose_threshold_by_cost(y, p, fn_cost=s['fn_cost'], fp_cost=s['fp_cost'])\n        rows.append({\n            'scenario': s['scenario'],\n            'fn_cost': s['fn_cost'],\n            'fp_cost': s['fp_cost'],\n            'recommended_threshold': float(best['threshold']),\n            'sensitivity_recall': float(best['recall_sensitivity']),\n            'specificity': float(best['specificity']),\n            'precision': float(best['precision']),\n            'flag_rate': float(best['flag_rate']),\n            'pr_auc': float(best['pr_auc']),\n            'roc_auc': float(best['roc_auc']),\n        })\n\n    policy_card = pd.DataFrame(rows)\n    display(policy_card.round(3))\n\n    out_path = (cnn_test_path.parent if cnn_test_path is not None else Path('../data')) / 'module_05c_model_policy_card.csv'\n    policy_card.to_csv(out_path, index=False)\n    print(f'Saved policy card to {out_path}')\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Wrap-up\n- PR and ROC should be interpreted together, not as substitutes.\n- Thresholds are operational choices tied to risk tolerance and staffing.\n- Hyperparameter selection should be evidence-based, not trial-and-error guesswork.\n- With a policy card, teams can discuss deployment using transparent assumptions."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}