{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 6: Neural Networks from Scratch (Interactive)\n",
    "## Brain-Inspired Intuition to Backpropagation\n",
    "\n",
    "**Goal:** Build intuition for neural networks from first principles using visual, interactive demos.\n",
    "\n",
    "This notebook is concept-first and intentionally light on math. You will control inputs with sliders and watch how predictions and learning change in real time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning objectives\n",
    "1. Explain how neural network ideas were inspired by biological neurons.\n",
    "2. Understand perceptron inputs, weights, bias, threshold, and output.\n",
    "3. Compare common activation functions and when they help.\n",
    "4. Build intuition for loss functions as \"what the model is trying to minimize.\"\n",
    "5. See backpropagation as a practical error-correction mechanism.\n",
    "6. Understand local minima vs global minima using a visual loss landscape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Clinical framing (Why this matters)\n",
    "Imagine a triage model that estimates patient risk from vitals and labs.\n",
    "- Inputs: heart rate, blood pressure, oxygen saturation, age, etc.\n",
    "- Output: probability of high-risk deterioration.\n",
    "\n",
    "A neural network is one way to map those inputs to a clinically useful prediction.\n",
    "Before using deep models in real medicine, you should understand these building blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "Run this once at the start. It handles local/Colab setup and dependency checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local runtime detected. Working directory: /Users/aaekay/Documents/projects/Medical-AI-101/chapters\n",
      "numpy ready: 2.3.0\n",
      "matplotlib ready: 3.10.3\n",
      "ipywidgets ready: 8.1.7\n",
      "plotly ready: 6.5.2\n",
      "nbformat ready: 5.10.4\n",
      "ipython ready: 9.3.0\n",
      "Plotly renderer set to: plotly_mimetype\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from importlib import import_module\n",
    "from importlib.metadata import PackageNotFoundError, version\n",
    "\n",
    "\n",
    "def setup_repo_for_colab(\n",
    "    repo_url='https://github.com/aaekay/Medical-AI-101.git',\n",
    "    repo_dir='/content/Medical-AI-101',\n",
    "    notebook_dir='chapters',\n",
    "):\n",
    "    if 'google.colab' not in sys.modules:\n",
    "        print(f'Local runtime detected. Working directory: {Path.cwd()}')\n",
    "        return\n",
    "\n",
    "    repo_path = Path(repo_dir)\n",
    "    if not repo_path.exists():\n",
    "        print('Cloning Medical-AI-101 into /content ...')\n",
    "        subprocess.check_call(['git', 'clone', repo_url, str(repo_path)])\n",
    "\n",
    "    target = repo_path / notebook_dir\n",
    "    os.chdir(target)\n",
    "    print(f'Colab ready. Working directory: {Path.cwd()}')\n",
    "\n",
    "\n",
    "def _parse_version(version_str):\n",
    "    parts = [int(p) for p in re.findall(r'\\d+', version_str)]\n",
    "    return tuple((parts + [0, 0, 0])[:3])\n",
    "\n",
    "\n",
    "def ensure_dependency(package_name, import_name=None, min_version=None):\n",
    "    import_name = import_name or package_name\n",
    "\n",
    "    needs_install = False\n",
    "    install_reason = ''\n",
    "\n",
    "    try:\n",
    "        import_module(import_name)\n",
    "    except ImportError:\n",
    "        needs_install = True\n",
    "        install_reason = 'not installed'\n",
    "\n",
    "    installed_version = None\n",
    "    if not needs_install:\n",
    "        try:\n",
    "            installed_version = version(package_name)\n",
    "        except PackageNotFoundError:\n",
    "            needs_install = True\n",
    "            install_reason = 'distribution metadata missing'\n",
    "\n",
    "    if not needs_install and min_version and installed_version:\n",
    "        if _parse_version(installed_version) < _parse_version(min_version):\n",
    "            needs_install = True\n",
    "            install_reason = f'version {installed_version} < required {min_version}'\n",
    "\n",
    "    requirement = f'{package_name}>={min_version}' if min_version else package_name\n",
    "\n",
    "    if needs_install:\n",
    "        print(f'Installing {requirement} ({install_reason}) ...')\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', requirement])\n",
    "        import_module(import_name)\n",
    "        installed_version = version(package_name)\n",
    "\n",
    "    print(f'{package_name} ready: {installed_version}')\n",
    "\n",
    "\n",
    "def configure_plotly_renderer():\n",
    "    import plotly.io as pio\n",
    "\n",
    "    renderers_obj = pio.renderers\n",
    "    available = set()\n",
    "\n",
    "    names = getattr(renderers_obj, 'names', None)\n",
    "    if names:\n",
    "        available = set(names)\n",
    "    elif hasattr(renderers_obj, 'keys'):\n",
    "        try:\n",
    "            available = set(renderers_obj.keys())\n",
    "        except Exception:\n",
    "            available = set()\n",
    "\n",
    "    if not available:\n",
    "        try:\n",
    "            available = set(list(renderers_obj))\n",
    "        except Exception:\n",
    "            available = set()\n",
    "\n",
    "    candidates = []\n",
    "    if 'google.colab' in sys.modules:\n",
    "        candidates.append('colab')\n",
    "    candidates.extend(['plotly_mimetype', 'notebook_connected', 'notebook', 'browser'])\n",
    "\n",
    "    renderer = next((name for name in candidates if name in available), None)\n",
    "    if renderer is None:\n",
    "        renderer = renderers_obj.default or 'browser'\n",
    "\n",
    "    try:\n",
    "        renderers_obj.default = renderer\n",
    "        print(f'Plotly renderer set to: {renderer}')\n",
    "    except Exception:\n",
    "        fallback = 'browser' if 'browser' in available else renderers_obj.default\n",
    "        renderers_obj.default = fallback\n",
    "        print(f'Plotly renderer set to fallback: {fallback}')\n",
    "\n",
    "\n",
    "setup_repo_for_colab()\n",
    "ensure_dependency('numpy')\n",
    "ensure_dependency('matplotlib')\n",
    "ensure_dependency('ipywidgets')\n",
    "ensure_dependency('plotly')\n",
    "ensure_dependency('nbformat', min_version='4.2.0')\n",
    "ensure_dependency('ipython', import_name='IPython')\n",
    "configure_plotly_renderer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "except ImportError as exc:\n",
    "    raise ImportError('ipywidgets is required for this notebook. Install with `pip install ipywidgets`.') from exc\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "COLOR_BG = '#f8fafc'\n",
    "COLOR_MAIN = '#2563eb'\n",
    "COLOR_ACCENT = '#f59e0b'\n",
    "COLOR_GOOD = '#059669'\n",
    "COLOR_BAD = '#dc2626'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: From Brain Neuron to AI Neuron\n",
    "Biological intuition:\n",
    "- **Dendrites** receive signals.\n",
    "- **Cell body (soma)** integrates signals.\n",
    "- If signal is strong enough, neuron **fires** through the axon.\n",
    "\n",
    "AI simplification:\n",
    "- Inputs are numbers.\n",
    "- We combine them into one score.\n",
    "- If score crosses a threshold, output activates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb5815afc4c4f7eab3ec70ab4110b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4, description='Exc A', max=10), IntSlider(value=3, description='Exc B',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def biological_to_ai_neuron_demo(excitatory_a=4, excitatory_b=3, inhibitory=2, threshold=5):\n",
    "    total_signal = excitatory_a + excitatory_b - inhibitory\n",
    "    fires = total_signal >= threshold\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(13, 4.5))\n",
    "    fig.patch.set_facecolor(COLOR_BG)\n",
    "\n",
    "    ax = axes[0]\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 6)\n",
    "    ax.axis('off')\n",
    "\n",
    "    soma = patches.Circle((5, 3), 1.1, facecolor='#fde68a', edgecolor='#92400e', linewidth=2)\n",
    "    ax.add_patch(soma)\n",
    "\n",
    "    ax.annotate('', xy=(4.1, 3.6), xytext=(1.2, 5.1), arrowprops=dict(arrowstyle='->', lw=3, color=COLOR_MAIN))\n",
    "    ax.annotate('', xy=(4.0, 2.7), xytext=(1.0, 3.0), arrowprops=dict(arrowstyle='->', lw=3, color=COLOR_MAIN))\n",
    "    ax.annotate('', xy=(4.1, 2.0), xytext=(1.3, 1.0), arrowprops=dict(arrowstyle='->', lw=3, color=COLOR_BAD))\n",
    "\n",
    "    ax.text(0.4, 5.2, f'+{excitatory_a}', fontsize=12, color=COLOR_MAIN, weight='bold')\n",
    "    ax.text(0.2, 3.1, f'+{excitatory_b}', fontsize=12, color=COLOR_MAIN, weight='bold')\n",
    "    ax.text(0.5, 0.8, f'-{inhibitory}', fontsize=12, color=COLOR_BAD, weight='bold')\n",
    "\n",
    "    axon_color = COLOR_GOOD if fires else '#6b7280'\n",
    "    ax.annotate('', xy=(9.2, 3), xytext=(6.1, 3), arrowprops=dict(arrowstyle='->', lw=5, color=axon_color))\n",
    "    ax.text(9.25, 3.05, 'Spike' if fires else 'No spike', color=axon_color, fontsize=11, va='center')\n",
    "\n",
    "    ax.text(4.3, 3.0, 'Soma', fontsize=11, color='#78350f')\n",
    "    ax.set_title('Biological neuron (intuition)', fontsize=13, weight='bold')\n",
    "\n",
    "    ax2 = axes[1]\n",
    "    labels = ['Excitatory A', 'Excitatory B', 'Inhibitory']\n",
    "    values = [excitatory_a, excitatory_b, -inhibitory]\n",
    "    colors = [COLOR_MAIN, COLOR_MAIN, COLOR_BAD]\n",
    "    ax2.bar(labels, values, color=colors)\n",
    "    ax2.axhline(0, color='black', linewidth=1)\n",
    "    ax2.axhline(threshold, color=COLOR_ACCENT, linestyle='--', linewidth=2, label=f'Threshold = {threshold}')\n",
    "    ax2.scatter(['Total'], [total_signal], color=COLOR_GOOD if fires else COLOR_BAD, s=120, zorder=3)\n",
    "    ax2.text(2.8, total_signal + 0.2, f'Total = {total_signal:.1f}', fontsize=11)\n",
    "    ax2.set_ylabel('Signal strength')\n",
    "    ax2.set_title('Signal integration', fontsize=13, weight='bold')\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    state = 'FIRES' if fires else 'DOES NOT FIRE'\n",
    "    display(Markdown(f\"**Integrated signal:** `{total_signal:.2f}`  |  **Threshold:** `{threshold:.2f}`  |  **Result:** `{state}`\"))\n",
    "\n",
    "\n",
    "widgets.interact(\n",
    "    biological_to_ai_neuron_demo,\n",
    "    excitatory_a=widgets.IntSlider(min=0, max=10, step=1, value=4, description='Exc A'),\n",
    "    excitatory_b=widgets.IntSlider(min=0, max=10, step=1, value=3, description='Exc B'),\n",
    "    inhibitory=widgets.IntSlider(min=0, max=10, step=1, value=2, description='Inhibitory'),\n",
    "    threshold=widgets.IntSlider(min=0, max=12, step=1, value=5, description='Threshold'),\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try this (Section 1)\n",
    "1. Increase inhibitory signal until the neuron no longer fires.\n",
    "2. Keep inputs fixed and move threshold up/down.\n",
    "3. Explain in one sentence: what does threshold represent clinically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Perceptron Basics (Inputs -> Weights -> Bias -> Threshold -> Output)\n",
    "A perceptron computes:\n",
    "`z = w1*x1 + w2*x2 + bias`\n",
    "Then applies a step rule:\n",
    "`output = 1 if z >= threshold else 0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3726ef3725584a6c9a59eaa843c63237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.5, description='x1', max=1.0, min=-1.0, step=0.05), FloatSlider(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def perceptron_playground(x1=0.5, x2=-0.2, w1=1.5, w2=-1.0, bias=0.1, threshold=0.0):\n",
    "    z = w1 * x1 + w2 * x2 + bias\n",
    "    y = 1 if z >= threshold else 0\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "    fig.patch.set_facecolor(COLOR_BG)\n",
    "\n",
    "    ax = axes[0]\n",
    "    ax.axis('off')\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 8)\n",
    "\n",
    "    for pos, label, val in [((1.5, 6), 'x1', x1), ((1.5, 2), 'x2', x2)]:\n",
    "        circ = patches.Circle(pos, 0.65, facecolor='#dbeafe', edgecolor='#1d4ed8', linewidth=2)\n",
    "        ax.add_patch(circ)\n",
    "        ax.text(pos[0], pos[1], f'{label}\\n{val:.2f}', ha='center', va='center', fontsize=10)\n",
    "\n",
    "    neuron = patches.Circle((6.2, 4), 1.25, facecolor='#fee2e2', edgecolor='#b91c1c', linewidth=2)\n",
    "    ax.add_patch(neuron)\n",
    "    ax.text(6.2, 4, 'Perceptron', ha='center', va='center', fontsize=11, weight='bold')\n",
    "\n",
    "    ax.annotate('', xy=(5.0, 4.9), xytext=(2.2, 6), arrowprops=dict(arrowstyle='->', lw=2.8, color=COLOR_MAIN))\n",
    "    ax.annotate('', xy=(5.0, 3.1), xytext=(2.2, 2), arrowprops=dict(arrowstyle='->', lw=2.8, color=COLOR_MAIN))\n",
    "    ax.text(3.2, 5.7, f'w1={w1:.2f}', color=COLOR_MAIN)\n",
    "    ax.text(3.2, 2.2, f'w2={w2:.2f}', color=COLOR_MAIN)\n",
    "\n",
    "    out_color = COLOR_GOOD if y == 1 else COLOR_BAD\n",
    "    ax.annotate('', xy=(9.1, 4), xytext=(7.45, 4), arrowprops=dict(arrowstyle='->', lw=4.0, color=out_color))\n",
    "    ax.text(9.15, 4.0, f'Output={y}', color=out_color, va='center', fontsize=11, weight='bold')\n",
    "\n",
    "    ax.text(4.6, 7.2, f'z = w1*x1 + w2*x2 + b = {z:.3f}', fontsize=11)\n",
    "    ax.text(4.6, 6.4, f'Threshold = {threshold:.3f}', fontsize=11, color=COLOR_ACCENT)\n",
    "    ax.set_title('Perceptron mechanics', fontsize=13, weight='bold')\n",
    "\n",
    "    ax2 = axes[1]\n",
    "    xx, yy = np.meshgrid(np.linspace(-1, 1, 220), np.linspace(-1, 1, 220))\n",
    "    zz = w1 * xx + w2 * yy + bias - threshold\n",
    "    ax2.contourf(xx, yy, zz >= 0, levels=[-1, 0, 1], colors=['#fee2e2', '#dcfce7'], alpha=0.75)\n",
    "\n",
    "    if abs(w2) > 1e-8:\n",
    "        x_line = np.linspace(-1, 1, 200)\n",
    "        y_line = (threshold - bias - w1 * x_line) / w2\n",
    "        ax2.plot(x_line, y_line, color='black', linewidth=2, label='Decision boundary')\n",
    "    elif abs(w1) > 1e-8:\n",
    "        x_const = (threshold - bias) / w1\n",
    "        ax2.axvline(x_const, color='black', linewidth=2, label='Decision boundary')\n",
    "\n",
    "    ax2.scatter([x1], [x2], color=out_color, s=110, edgecolor='black', linewidth=1.2, zorder=3)\n",
    "    ax2.set_xlim(-1, 1)\n",
    "    ax2.set_ylim(-1, 1)\n",
    "    ax2.set_xlabel('x1')\n",
    "    ax2.set_ylabel('x2')\n",
    "    ax2.set_title('Classification region (step output)', fontsize=13, weight='bold')\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "widgets.interact(\n",
    "    perceptron_playground,\n",
    "    x1=widgets.FloatSlider(min=-1, max=1, step=0.05, value=0.5, description='x1'),\n",
    "    x2=widgets.FloatSlider(min=-1, max=1, step=0.05, value=-0.2, description='x2'),\n",
    "    w1=widgets.FloatSlider(min=-3, max=3, step=0.1, value=1.5, description='w1'),\n",
    "    w2=widgets.FloatSlider(min=-3, max=3, step=0.1, value=-1.0, description='w2'),\n",
    "    bias=widgets.FloatSlider(min=-2, max=2, step=0.1, value=0.1, description='bias'),\n",
    "    threshold=widgets.FloatSlider(min=-2, max=2, step=0.1, value=0.0, description='threshold'),\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try this (Section 2)\n",
    "1. Set `w1` close to zero and observe how `x1` loses influence.\n",
    "2. Increase bias while keeping inputs fixed. What happens to output?\n",
    "3. Move threshold higher and explain the clinical meaning (more conservative trigger)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Activation Functions\n",
    "Step functions are rigid. Modern neural networks use smooth activations so learning is easier.\n",
    "\n",
    "Common activations:\n",
    "- Sigmoid: output between 0 and 1\n",
    "- Tanh: output between -1 and 1\n",
    "- ReLU: zero for negative inputs, linear for positive inputs\n",
    "- Leaky ReLU: small negative slope to avoid dead neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52bc74faccfd42b38f955c86da357d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Function', index=1, options=('Step', 'Sigmoid', 'Tanh', 'ReLU', 'L…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def activation_values(name, x, alpha=0.1):\n",
    "    if name == 'Step':\n",
    "        return (x >= 0).astype(float), 'f(z)=1 if z>=0 else 0'\n",
    "    if name == 'Sigmoid':\n",
    "        return 1 / (1 + np.exp(-x)), 'f(z)=1/(1+exp(-z))'\n",
    "    if name == 'Tanh':\n",
    "        return np.tanh(x), 'f(z)=tanh(z)'\n",
    "    if name == 'ReLU':\n",
    "        return np.maximum(0, x), 'f(z)=max(0,z)'\n",
    "    if name == 'Leaky ReLU':\n",
    "        return np.where(x >= 0, x, alpha * x), f'f(z)=z if z>=0 else {alpha:.2f}*z'\n",
    "    raise ValueError('Unknown activation name')\n",
    "\n",
    "\n",
    "def activation_explorer(activation='Sigmoid', z_point=0.5, alpha=0.1):\n",
    "    x = np.linspace(-6, 6, 500)\n",
    "    y, formula = activation_values(activation, x, alpha=alpha)\n",
    "    y_point, _ = activation_values(activation, np.array([z_point]), alpha=alpha)\n",
    "    y_point = float(y_point[0])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 4.5))\n",
    "    fig.patch.set_facecolor(COLOR_BG)\n",
    "    ax.plot(x, y, color=COLOR_MAIN, linewidth=3)\n",
    "    ax.scatter([z_point], [y_point], color=COLOR_ACCENT, s=100, zorder=5)\n",
    "    ax.axvline(0, color='gray', linestyle='--', linewidth=1)\n",
    "    ax.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "    ax.set_title(f'{activation} activation', fontsize=14, weight='bold')\n",
    "    ax.set_xlabel('Input z')\n",
    "    ax.set_ylabel('Output f(z)')\n",
    "    ax.text(0.02, 0.95, formula, transform=ax.transAxes, va='top', fontsize=11,\n",
    "            bbox=dict(facecolor='white', edgecolor='#cbd5e1', boxstyle='round,pad=0.3'))\n",
    "    ax.text(0.02, 0.84, f'At z={z_point:.2f}, output={y_point:.4f}', transform=ax.transAxes, va='top', fontsize=11)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "widgets.interact(\n",
    "    activation_explorer,\n",
    "    activation=widgets.Dropdown(options=['Step', 'Sigmoid', 'Tanh', 'ReLU', 'Leaky ReLU'], value='Sigmoid', description='Function'),\n",
    "    z_point=widgets.FloatSlider(min=-6, max=6, step=0.1, value=0.5, description='z'),\n",
    "    alpha=widgets.FloatSlider(min=0.01, max=0.4, step=0.01, value=0.1, description='alpha'),\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try this (Section 3)\n",
    "1. Compare sigmoid vs ReLU at large positive and negative `z`.\n",
    "2. Use Leaky ReLU and change `alpha`; observe negative-side behavior.\n",
    "3. Why might a hard step function be harder to train than sigmoid/ReLU?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Loss Functions (How wrong are we?)\n",
    "Loss is a number that measures prediction error. Training tries to reduce this number.\n",
    "\n",
    "- Regression examples: **MAE**, **MSE**\n",
    "- Classification examples: **Binary Cross-Entropy (BCE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757e5186a0a84c1cb0fd22f6c94fccdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.7, description='True y', max=1.0, step=0.01), FloatSlider(value=0.4,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def regression_loss_demo(true_value=0.7, pred_value=0.4):\n",
    "    x = np.linspace(0, 1, 400)\n",
    "    mse_curve = (x - true_value) ** 2\n",
    "    mae_curve = np.abs(x - true_value)\n",
    "\n",
    "    mse = (pred_value - true_value) ** 2\n",
    "    mae = abs(pred_value - true_value)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 4.5))\n",
    "    fig.patch.set_facecolor(COLOR_BG)\n",
    "    ax.plot(x, mse_curve, color='#7c3aed', linewidth=2.5, label='MSE')\n",
    "    ax.plot(x, mae_curve, color='#16a34a', linewidth=2.5, label='MAE')\n",
    "    ax.scatter([pred_value], [mse], color='#7c3aed', s=90)\n",
    "    ax.scatter([pred_value], [mae], color='#16a34a', s=90)\n",
    "    ax.axvline(true_value, color=COLOR_ACCENT, linestyle='--', linewidth=2, label=f'True value={true_value:.2f}')\n",
    "    ax.set_title('Regression losses vs prediction', fontsize=13, weight='bold')\n",
    "    ax.set_xlabel('Prediction')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    display(Markdown(f\"**MAE:** `{mae:.4f}` | **MSE:** `{mse:.4f}`\"))\n",
    "\n",
    "\n",
    "widgets.interact(\n",
    "    regression_loss_demo,\n",
    "    true_value=widgets.FloatSlider(min=0, max=1, step=0.01, value=0.7, description='True y'),\n",
    "    pred_value=widgets.FloatSlider(min=0, max=1, step=0.01, value=0.4, description='Pred y'),\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81aa7b42ffc94ad1836e142fa0fab981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(ToggleButtons(description='True class', index=1, options=(0, 1), value=1), FloatSlider(v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def classification_loss_demo(true_label=1, pred_prob=0.8):\n",
    "    eps = 1e-9\n",
    "    p = np.linspace(0.001, 0.999, 500)\n",
    "    bce_y1 = -np.log(p)\n",
    "    bce_y0 = -np.log(1 - p)\n",
    "\n",
    "    y = int(true_label)\n",
    "    bce = -(y * np.log(pred_prob + eps) + (1 - y) * np.log(1 - pred_prob + eps))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 4.5))\n",
    "    fig.patch.set_facecolor(COLOR_BG)\n",
    "    ax.plot(p, bce_y1, color='#2563eb', linewidth=2.5, label='BCE when true label=1')\n",
    "    ax.plot(p, bce_y0, color='#dc2626', linewidth=2.5, label='BCE when true label=0')\n",
    "    ax.scatter([pred_prob], [bce], color=COLOR_ACCENT, s=120, zorder=5)\n",
    "    ax.set_ylim(0, 6)\n",
    "    ax.set_xlabel('Predicted probability for class 1')\n",
    "    ax.set_ylabel('BCE loss')\n",
    "    ax.set_title('Binary cross-entropy intuition', fontsize=13, weight='bold')\n",
    "    ax.legend(loc='upper center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    display(Markdown(f\"**True label:** `{y}` | **Pred prob:** `{pred_prob:.3f}` | **BCE:** `{bce:.4f}`\"))\n",
    "\n",
    "\n",
    "widgets.interact(\n",
    "    classification_loss_demo,\n",
    "    true_label=widgets.ToggleButtons(options=[0, 1], value=1, description='True class'),\n",
    "    pred_prob=widgets.FloatSlider(min=0.001, max=0.999, step=0.001, value=0.8, description='Pred prob'),\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try this (Section 4)\n",
    "1. In regression, move prediction away from true value and compare MAE vs MSE sensitivity.\n",
    "2. In BCE, set true label to 1 and push predicted probability toward 0. What happens?\n",
    "3. Why might wrong-but-confident predictions be dangerous in medicine?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Backpropagation (Error correction)\n",
    "High-level idea:\n",
    "1. Make prediction\n",
    "2. Compute loss\n",
    "3. Compute gradients (how each parameter affected error)\n",
    "4. Update weights/bias in the opposite direction of gradient\n",
    "\n",
    "In this demo, we train a **single sigmoid neuron** using binary cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4103109781545c9b5e263e692e0e3bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.7, description='x1', max=1.0, step=0.01), FloatSlider(value=0.2, des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def run_single_neuron_training(x1, x2, y_true, w1, w2, b, lr, steps):\n",
    "    eps = 1e-9\n",
    "    history = []\n",
    "\n",
    "    for step in range(steps + 1):\n",
    "        z = w1 * x1 + w2 * x2 + b\n",
    "        y_hat = sigmoid(z)\n",
    "        loss = -(y_true * np.log(y_hat + eps) + (1 - y_true) * np.log(1 - y_hat + eps))\n",
    "\n",
    "        history.append({\n",
    "            'step': step,\n",
    "            'w1': w1,\n",
    "            'w2': w2,\n",
    "            'b': b,\n",
    "            'y_hat': y_hat,\n",
    "            'loss': loss,\n",
    "        })\n",
    "\n",
    "        if step == steps:\n",
    "            break\n",
    "\n",
    "        dz = y_hat - y_true\n",
    "        dw1 = dz * x1\n",
    "        dw2 = dz * x2\n",
    "        db = dz\n",
    "\n",
    "        w1 = w1 - lr * dw1\n",
    "        w2 = w2 - lr * dw2\n",
    "        b = b - lr * db\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def backprop_demo(x1=0.7, x2=0.2, y_true=1, w1=-0.5, w2=0.6, b=0.0, lr=0.4, steps=20):\n",
    "    history = run_single_neuron_training(x1, x2, y_true, w1, w2, b, lr, steps)\n",
    "\n",
    "    losses = [h['loss'] for h in history]\n",
    "    step_idx = [h['step'] for h in history]\n",
    "\n",
    "    start = history[0]\n",
    "    end = history[-1]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(13, 4.8))\n",
    "    fig.patch.set_facecolor(COLOR_BG)\n",
    "\n",
    "    axes[0].plot(step_idx, losses, color=COLOR_MAIN, linewidth=2.8)\n",
    "    axes[0].scatter([0, steps], [start['loss'], end['loss']], color=[COLOR_BAD, COLOR_GOOD], s=90)\n",
    "    axes[0].set_title('Loss decreases as we update parameters', fontsize=13, weight='bold')\n",
    "    axes[0].set_xlabel('Training step')\n",
    "    axes[0].set_ylabel('BCE loss')\n",
    "\n",
    "    labels = ['w1', 'w2', 'bias']\n",
    "    start_params = [start['w1'], start['w2'], start['b']]\n",
    "    end_params = [end['w1'], end['w2'], end['b']]\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.33\n",
    "\n",
    "    axes[1].bar(x - width / 2, start_params, width=width, label='Start', color='#93c5fd')\n",
    "    axes[1].bar(x + width / 2, end_params, width=width, label='After training', color='#34d399')\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(labels)\n",
    "    axes[1].set_title('Parameter updates from backpropagation', fontsize=13, weight='bold')\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    summary = (\n",
    "        f\"**Before training**: prediction=`{start['y_hat']:.4f}`, loss=`{start['loss']:.4f}`  \\\n",
    "\"\n",
    "        f\"**After {steps} steps**: prediction=`{end['y_hat']:.4f}`, loss=`{end['loss']:.4f}`  \\\n",
    "\"\n",
    "        \"Update rule intuition: `parameter_new = parameter_old - learning_rate * gradient`\"\n",
    "    )\n",
    "    display(Markdown(summary))\n",
    "\n",
    "\n",
    "widgets.interact(\n",
    "    backprop_demo,\n",
    "    x1=widgets.FloatSlider(min=0, max=1, step=0.01, value=0.7, description='x1'),\n",
    "    x2=widgets.FloatSlider(min=0, max=1, step=0.01, value=0.2, description='x2'),\n",
    "    y_true=widgets.ToggleButtons(options=[0, 1], value=1, description='True y'),\n",
    "    w1=widgets.FloatSlider(min=-2, max=2, step=0.05, value=-0.5, description='w1'),\n",
    "    w2=widgets.FloatSlider(min=-2, max=2, step=0.05, value=0.6, description='w2'),\n",
    "    b=widgets.FloatSlider(min=-2, max=2, step=0.05, value=0.0, description='bias'),\n",
    "    lr=widgets.FloatSlider(min=0.01, max=1.0, step=0.01, value=0.4, description='learn rate'),\n",
    "    steps=widgets.IntSlider(min=1, max=100, step=1, value=20, description='steps'),\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try this (Section 5)\n",
    "1. Start with a very high learning rate (`~1.0`) and observe instability.\n",
    "2. Use a very low learning rate (`~0.01`) and observe slow learning.\n",
    "3. Change true label from 1 to 0 and explain how gradient direction changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Local Minima vs Global Minima\n",
    "Neural network optimization searches for low-loss regions.\n",
    "- **Global minimum:** best loss overall.\n",
    "- **Local minimum:** good in a local neighborhood, but not best overall.\n",
    "\n",
    "Real deep networks are high-dimensional; this 2D surface is a visual intuition tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fafcf29988948f7a2b379a52ed0ad97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-1.8, description='start w', max=2.2, min=-2.2), FloatSlider(value=1.5…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def landscape_loss(w, b):\n",
    "    return 0.25 * (w ** 4 + b ** 4) - 1.2 * (w ** 2 + b ** 2) + 0.5 * w * b + 2.0\n",
    "\n",
    "\n",
    "def landscape_grad(w, b):\n",
    "    d_w = w ** 3 - 2.4 * w + 0.5 * b\n",
    "    d_b = b ** 3 - 2.4 * b + 0.5 * w\n",
    "    return d_w, d_b\n",
    "\n",
    "\n",
    "def descent_path(start_w, start_b, lr, steps):\n",
    "    w, b = start_w, start_b\n",
    "    path = [(w, b, landscape_loss(w, b))]\n",
    "    for _ in range(steps):\n",
    "        d_w, d_b = landscape_grad(w, b)\n",
    "        w = w - lr * d_w\n",
    "        b = b - lr * d_b\n",
    "        path.append((w, b, landscape_loss(w, b)))\n",
    "    return np.array(path)\n",
    "\n",
    "\n",
    "def minima_demo(start_w=-1.8, start_b=1.5, lr=0.05, steps=30):\n",
    "    w_axis = np.linspace(-2.4, 2.4, 220)\n",
    "    b_axis = np.linspace(-2.4, 2.4, 220)\n",
    "    W, B = np.meshgrid(w_axis, b_axis)\n",
    "    Z = landscape_loss(W, B)\n",
    "\n",
    "    path = descent_path(start_w, start_b, lr, steps)\n",
    "\n",
    "    g_idx = np.argmin(Z)\n",
    "    g_w = W.ravel()[g_idx]\n",
    "    g_b = B.ravel()[g_idx]\n",
    "    g_loss = Z.ravel()[g_idx]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 7))\n",
    "    fig.patch.set_facecolor(COLOR_BG)\n",
    "\n",
    "    contour = ax.contourf(W, B, Z, levels=40, cmap='viridis')\n",
    "    plt.colorbar(contour, ax=ax, label='Loss')\n",
    "    ax.contour(W, B, Z, levels=12, colors='white', linewidths=0.5, alpha=0.5)\n",
    "\n",
    "    ax.plot(path[:, 0], path[:, 1], color='#f97316', linewidth=2.5, marker='o', markersize=3, label='Gradient descent path')\n",
    "    ax.scatter(path[0, 0], path[0, 1], color='#ef4444', s=90, edgecolor='black', linewidth=1, label='Start')\n",
    "    ax.scatter(path[-1, 0], path[-1, 1], color='#22c55e', s=90, edgecolor='black', linewidth=1, label='End')\n",
    "    ax.scatter(g_w, g_b, color='gold', marker='*', s=220, edgecolor='black', linewidth=1, label='Approx global minimum')\n",
    "\n",
    "    ax.set_xlabel('Parameter w')\n",
    "    ax.set_ylabel('Parameter b')\n",
    "    ax.set_title('Loss landscape: local vs global minima intuition', fontsize=13, weight='bold')\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    end_loss = path[-1, 2]\n",
    "    display(Markdown(f\"Start loss: `{path[0,2]:.4f}` | End loss: `{end_loss:.4f}` | Approx global minimum loss (grid): `{g_loss:.4f}`\"))\n",
    "\n",
    "\n",
    "widgets.interact(\n",
    "    minima_demo,\n",
    "    start_w=widgets.FloatSlider(min=-2.2, max=2.2, step=0.1, value=-1.8, description='start w'),\n",
    "    start_b=widgets.FloatSlider(min=-2.2, max=2.2, step=0.1, value=1.5, description='start b'),\n",
    "    lr=widgets.FloatSlider(min=0.005, max=0.2, step=0.005, value=0.05, description='learn rate'),\n",
    "    steps=widgets.IntSlider(min=1, max=80, step=1, value=30, description='steps'),\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try this (Section 6)\n",
    "1. Keep learning rate fixed and move only the starting point. Do you always end at the same place?\n",
    "2. Increase learning rate and observe overshooting/divergence patterns.\n",
    "3. In one sentence: why can initialization matter in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "You now have the core mental model:\n",
    "1. Neuron inspiration -> perceptron mechanics\n",
    "2. Activation functions make networks expressive and trainable\n",
    "3. Loss measures error\n",
    "4. Backpropagation adjusts parameters to reduce loss\n",
    "5. Optimization can get trapped in local minima\n",
    "\n",
    "Next step in the curriculum: stack many neurons into hidden layers and train a small network end-to-end on a medical dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
